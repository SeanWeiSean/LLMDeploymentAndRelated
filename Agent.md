# 模型部署 Agent 设计与实践

## 背景

从4月到12月，模型的全生命周期管理已经形成了一套内部最佳实践，涵盖了模型的测试、部署、压测、评估，以及上线后的监控和动态扩缩容。在这一系列流程中，我们自然而然地想到一个问题：**能不能用 LLM 来辅助完成这些重复性工作？**

基于这个想法，我们开始设计一套管理 Agent。目前市面上已经有比较成熟的 LangChain 框架，但我们选用了微软开源的 **Semantic Kernel**，主要出于合规考量和对 C# 的良好支持。结合已有的模型部署和测试经验，我们设计了一套 Agent 来辅助完成模型的部署和测试工作。

## LLMOps 流程概览

这几个月构建的 LLMOps 流程主要分为以下几个阶段：

1. **模型测试**：确保模型在给定的 GPU 型号、CUDA 版本、推理框架下能够正常启动且算子兼容
2. **模型部署**：完成模型的容器化部署，在负载均衡器注册模型和对应的合规服务
3. **模型压测**：标准化压测，收集模型的吞吐量、延迟以及下游场景中的特殊指标
4. **模型评估**：进行标准化评估，收集模型在 MMLU_PRO、AME2025 等标准评测集上的表现，注册到模型评测平台
5. **模型上线与监控**：开放流量，根据工单、下游反馈及日志解决线上可用性问题

## 设计思考

### 核心原则

回顾整个任务设计，最重要的是两点：

- **Agent 开发者必须懂自己的任务在干什么**
- **Agent 开发者必须掌握 LLM 的局限性和优势**

在这两者基础上，设计出合理的任务粒度和 prompt，才能让 Agent 真正完成任务。

### 理想与现实

在我理想的场景中，Claude Code 和 Manus 就是 Agent 的最终形态——给定几个大方向的入口，它就能真正地读写、思考。但在这轮选型中，我们还是对 Agent 的边界做了很多限制。在专有业务场景下，**成功率、稳定性、可解释性、Token 消耗量**比通用能力更重要。

### Agent vs Workflow 的边界

在第一版做出来的时候，我遇到了一个很多人做 Agent 都会思考的问题：

> 我这个 Agent 和 Workflow 到底有什么区别？它只是在受限范围内的状态机，写成可执行代码又有什么区别？

基于这个判断方法，我们把可以固化为可执行代码的步骤换成了 **MCP + Workflow**，只留下最精简的 Agent 部分——那些真正需要 LLM 决策下一步动向的环节。

## 架构设计：MCP + Workflow + Agent

经过几轮试错，最终的设计从"Agent 全自主"演变为 **MCP + Workflow + Agent 混合架构**。

### 为什么选择 MCP？

插件模式的MCP（Model Context Protocol）在这套架构中承担了关键角色：

| 优势 | 说明 |
|------|------|
| **缓解幻觉问题** | Workflow 和 MCP 通过代码约束 LLM 的输出边界，减少幻觉；MCP 的显式调用记录也能辅助用户判断内容是否可信 |
| **缓解上下文压力** | 使 LLM 聚焦在最近的 10K Token 上，每次只需关心当前指令，无需 LLM 总结先前内容 |
| **易于集成** | 可作为 GitHub Copilot 插件发布，用户简单安装即可获得部署能力 |
| **权限隔离** | 复用用户本地权限，避免 Agent 权限过大带来的安全隐患 |

### 权限设计的考量

举个例子：如果做成纯 Agent 查询日志，我们要么让 Agent 拥有访问所有数据库的权限，要么让用户带上 Token 从我们的服务中转一次。服务中转会带来额外问题：

- 要不要追踪用户的查询行为？
- 如何判断异常流量？
- 如何做流量截留？

我们的策略是：**能复用现有平台策略，就不额外造轮子。** VSCode插件 随着用户的权限走，天然就是权限隔离的。

## 工作拆解

基于上述设计，我们的工作分为三块：

1. **API 开发与集成**：开发或利用现有 API 接口供 MCP 调用。其中压测和评估 API 需要自主开发，部署和查询服务有现成 API 可用
2. **MCP 工具开发**：串联 Workflow 和 Agent 任务，让用户通过命令行或 UI 触发完整流程
3. **Agent Prompt 与任务设计**：设计 Agent 的 prompt 和任务边界

## Workflow 节点设计

在 Workflow 中，我们设计了几个典型的任务节点：

1. **模型测试节点**
2. **模型压测节点**
3. **日志总结节点**

模型测试和压测是典型的重复性工作：给定模型名称和地址，跑一套预定义的测试集和压测集，收集结果并输出指标。我们把这套流程做成微服务发布到内网，接入 MCP，通过用户显式触发。在信息孤岛还没完全打通的情况下，**人的输入能够帮助整套流程更加顺畅**。

## MCP 工具设计：上层抽象的重要性

MCP 工具开发需要开发者有一定的"品味"，做好足够的上层抽象。上层抽象能让 LLM 不至于淹没在几百个工具里丢失上下文。

### MCP Discovery 机制

在工具还不多的早期，我就坚持引入了 **MCP Discovery** 的理念：把工具分成几个大类工具包，隐藏在 `activate_deploy_tools`、`activate_test_tools` 这些上层工具里。

LLM 在收到用户请求时，会先调用 `activate_deploy_tools`，它会展开相关工具，按需暴露内部工具到 MCP 列表里。用户需要手动确认开启这些工具，同时其他 MCP 工具依然隐藏在各自的工具包内，不干扰当前上下文。

这一步主要是**降低 LLM 的理解负担**。很多工具在各自语境里功能清晰，但放在 MCP 总列表里，命名和介绍容易混淆——比如 `deploy_to_azure` 可能是 Azure 官方 MCP 工具，也可能是某个用户定制的带特殊步骤的部署流程。

### 把作文题变成填空题

在部署 MCP 中会涉及让模型写 YAML 部署文件。比起让 LLM 一次性写对整篇"作文"，我们的办法是**把作文题变成填空题**——输出越少，错得越少。这是牺牲一定可扩展性换来的成功率提升。

## Agent 任务：处理不可预知性

在整个流程中，最难 Workflow 化的就是**模型部署**。它包含了太多不可预知的因素：

- 模型格式
- 模型依赖
- 启动参数
- 运行环境

这些都是目前无法完全标准化的东西。我们选择把它做成 Agent 任务，带着过去一年收集的经验作为 prompt，让 Agent 收集信息，逐步尝试部署模型。

### 本地测试流程

虽然过程是开放式的，但我们设计了边界和任务指导，帮助 Agent 尽快收敛到正确结果：

1. 查询本地硬件状态
2. 阅读模型的部署说明
3. 下载模型权重
4. 选择推理框架
5. 下载镜像
6. 启动容器
7. 调试结果并完成任务

### 常见问题

在这个过程中，每一步都可能遇到边界问题：

- 下载中断
- 推理框架版本不一致
- 推理框架选择错误
- 容器启动失败

但这个过程又不宜限制过死，否则会让 Agent 失去探索能力，退化成 Workflow。

## 未来展望

整个流程中最欠缺的是**总结和学习能力**。每次重新拉起部署服务，Agent 就会忘记过去的经验。

未来的设计中，我们计划给 Agent 加上一个**经验库**，把每次部署的经验和教训记录下来，让 Agent 在未来任务中能够参考历史经验，避免重复犯错。

## 总结

这套 **Agent + Workflow + MCP** 的混合架构，已经能够在一定程度上辅助我们完成模型的部署和测试工作。
<img width="715" height="915" alt="image" src="https://github.com/user-attachments/assets/ef8ed383-cae3-487b-baa1-70a46bbeaa3c" />

### 待解决的问题

但问题依然明显且尚未解决：

- **Token 消耗过大**：日志查询对 Prefill Token 的用量非常大，很容易花光预算。如何正确处理 Token 和总结是一个挑战
- **系统集成不完整**：整套 MCP 作为启动入口，还没有和内部的 OnCall 系统打通，无法完成线上日志和报警的自动分析与总结
- **MCP 抽象仍有不足**：部分工具的抽象层次还需优化
- **模型依赖性强**：MCP 服务强依赖于底层模型的可用性
- **Agent 不确定性**：Agent 任务的成功率和结果仍有波动

<img width="2096" height="225" alt="image" src="https://github.com/user-attachments/assets/e0eac428-f9f0-4e9d-946e-a76875a44d28" />

好用不好用？等我们 LLM Serving Team 自己先用一个月再来反馈吧。
