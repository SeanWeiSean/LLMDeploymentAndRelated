# LLMDeploymentAndRelated

模型部署工作已经开展了4个月了,从4张A10，T4到200+ H20和A100,部署了十余家家国内厂商的模型，遇到了很多杂乱的问题，用这个md来做一个陆陆续续的总结记录这半年，事情太多太杂，记一笔给以后留下点旧经验。
1.推理框架 vllm
2.打镜像 cuda版本与gpu分配 docker
3.从裸机到容器服务 k8s
4.转发和网络 SSE
5.在线服务设计
6.压力测试和配额管理
7.通信瓶颈与计算瓶颈
8.微调 LlamaFactory


6/24
##推理框架
工作的突增主要是在2025年过年deepseek出来之后，公司部门要求尽快对deepseek的模型进行评估，在那之前，我只是用组内刚化缘拿到单卡A100在内部平台上进行一些基础推理，那时候用的Llama系列在huggingface提供的transformer包，封装了简单的推理逻辑和基础http service，那时候对streaming,SSE,latency,TTFT都没有什么概念，就是知道反正有一段输入就能有一段输出就够了。作为之前没有参与过模型训练和部署的人来说，有几个关键概念是在部署模型之后才开始理解的。
###Base模型和Instruct微调模型
在最开始huggingface琳琅满目的模型中，自然而言抽取到的是各个模型家族的base模型，在最开始的部署当中，我发现base模型和在chatgpt官网上提供的模型结果天差地别，它很容易陷入死循环，本身的输出也没有解决任何问题，后面它在国内有了一个很贴切的翻译，叫做基座模型，它也组成了我对LLM的初步理解，LLM是在根据Prefill阶段的结果预测下一个Token。一个很简单的例子，在Base模型里发送内容为“你好”的Query,它的返回会是“你好，这是一条深幽的小路，小路上...”这种通顺的话，它本身不会回答问题。与之对应的是Instruct模型，它能够跟踪提问者的问题并针对性的给出响应，让模型能够听懂人类提问的意图，在Instruct模型里发送内容为“你好”的Query，它的返回基本都是“你好！我是blabl模型，今天有什么可以帮你”不再是单纯的续写提问，而是开始了对话。
###标准输入输出接口
在闭门造车的时候，根本没有想过接口格式问题，就算如此，在最开始的阶段，也和我们下游demo的同事花了一阵子时间在对其接口格式上，目前还比较常用的接口是 completion和chatcompletion接口, Body分别是prompt和message，如果让我来面试部署测试的小伙伴，让他写出一个合法的body会是我最愿意提问的第一个问题，它直接反应了面试者有没有亲手写过query，抄过无数个query之后，Model和Message的格式基本就很难被忘记了。
###推理框架
很快我们就从闭门造车接入了国际正规，引入了vLLM,这是一个标准化且广泛使用的推理框架，它支持大部分市面上的开源模型，支持张量并行和流水线并行并保持超高的吞吐量，在批量推理的过程里，我推荐参考这篇文章对vLLM的内部原理有一些初步理解。https://www.runpod.io/blog/introduction-to-vllm-and-pagedattention![image](https://github.com/user-attachments/assets/baa7bbd4-597b-4d09-a7f4-57d57645cee9)对于部署的用户来说，安装环境，使用docker拉下最新的vLLM的框架，在github vLLM issue里查看模型的支持情况，基本就能起一个服务了。
